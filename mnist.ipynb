{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All needed imports to run notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "tf.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "num_classes = 10\n",
    "input_shape = (28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cache images in memory before shuffling for better performance\n",
    "# ds_train = ds_train.cache()\n",
    "# # for true randomness, set shuffle buffer to full dataset size\n",
    "# # standard value is 1000 if whole dataset doesn't fit in memory\n",
    "# ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "# # get unique batch at each epoch\n",
    "# ds_train = ds_train.batch(128)\n",
    "# ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize images\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "# # convert class vectors to binary class matrices\n",
    "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "# y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build evaluation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_test = ds_test.map(\n",
    "#     normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "# )\n",
    "# ds_test = ds_test.batch(128)\n",
    "# ds_test = ds_test.cache()\n",
    "# ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# note, our objects are `one-hot` which means objects can belong to only one class\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(128,activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes)\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_12 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.3702 - sparse_categorical_accuracy: 0.8981 - val_loss: 0.1748 - val_sparse_categorical_accuracy: 0.9542\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1761 - sparse_categorical_accuracy: 0.9495 - val_loss: 0.1329 - val_sparse_categorical_accuracy: 0.9625\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1294 - sparse_categorical_accuracy: 0.9631 - val_loss: 0.1065 - val_sparse_categorical_accuracy: 0.9703\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1013 - sparse_categorical_accuracy: 0.9703 - val_loss: 0.0968 - val_sparse_categorical_accuracy: 0.9730\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.0833 - sparse_categorical_accuracy: 0.9761 - val_loss: 0.0913 - val_sparse_categorical_accuracy: 0.9753\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.0698 - sparse_categorical_accuracy: 0.9803 - val_loss: 0.0889 - val_sparse_categorical_accuracy: 0.9742\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.0583 - sparse_categorical_accuracy: 0.9833 - val_loss: 0.0826 - val_sparse_categorical_accuracy: 0.9757\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.0509 - sparse_categorical_accuracy: 0.9850 - val_loss: 0.0808 - val_sparse_categorical_accuracy: 0.9772\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.0428 - sparse_categorical_accuracy: 0.9879 - val_loss: 0.0777 - val_sparse_categorical_accuracy: 0.9780\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.0365 - sparse_categorical_accuracy: 0.9897 - val_loss: 0.0777 - val_sparse_categorical_accuracy: 0.9778\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.0317 - sparse_categorical_accuracy: 0.9913 - val_loss: 0.0783 - val_sparse_categorical_accuracy: 0.9798\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.0261 - sparse_categorical_accuracy: 0.9933 - val_loss: 0.0798 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.0231 - sparse_categorical_accuracy: 0.9942 - val_loss: 0.0813 - val_sparse_categorical_accuracy: 0.9787\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.0207 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.0825 - val_sparse_categorical_accuracy: 0.9758\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.0177 - sparse_categorical_accuracy: 0.9957 - val_loss: 0.0799 - val_sparse_categorical_accuracy: 0.9782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff7405a3240>"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=128\n",
    "epochs=15\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.07264316827058792. Test accuracy: 0.9781000018119812\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(ds_test, verbose=0)\n",
    "print(f'Test loss: {score[0]}. Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 9 6 6 8 3 9 1 4 7]\n",
      "[7 9 6 6 9 3 9 1 4 7]\n"
     ]
    }
   ],
   "source": [
    "indices = [i for i, x in enumerate(x_test) if i % 1000 == 0]\n",
    "x = x_test[indices]\n",
    "y = y_test[indices]\n",
    "\n",
    "predictions = model.predict(x)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "print(predictions)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
